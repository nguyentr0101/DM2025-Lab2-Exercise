{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name Truong Ngoc Khoi Nguyen\n",
    "\n",
    "Student ID: D142113032\n",
    "\n",
    "GitHub ID: nguyentr0101\n",
    "\n",
    "Kaggle name: truongngockhoinguyen\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:30:28.657287Z",
     "start_time": "2025-11-30T02:30:28.650495Z"
    }
   },
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==================\n",
    "# NLTK DATA DOWNLOAD\n",
    "# ==================\n",
    "import nltk\n",
    "nltk.download('punkt') # download the NLTK datasets\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "print(\"NLTK data downloaded successfully.\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Scikit-learn imports for vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:39:09.721083Z",
     "start_time": "2025-11-30T02:39:09.003749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===================\n",
    "# 1. DATA PREPARATION\n",
    "# ===================\n",
    "# 1.1 Load data_identification.csv (train/test split)\n",
    "data_identification = pd.read_csv(\"data/data_identification.csv\")\n",
    "print(f\"data_identification shape: {data_identification.shape}\")\n",
    "\n",
    "# 1.2 Load emotion.csv (labels for training data)\n",
    "emotion_df = pd.read_csv(\"data/emotion.csv\")\n",
    "print(f\"emotion_df shape: {emotion_df.shape}\")\n",
    "\n",
    "# 1.3 Load final_posts.json (text data)\n",
    "with open(\"data/final_posts.json\", 'r', encoding='utf-8') as f:\n",
    "    posts_json = json.load(f)\n",
    "\n",
    "# Extract posts into a DataFrame (similar to Lab 1 dictionary to dataframe)\n",
    "posts_data = []\n",
    "for item in posts_json:\n",
    "    post = item['root']['_source']['post']\n",
    "    posts_data.append({\n",
    "        'id': post['post_id'],\n",
    "        'text': post['text'],\n",
    "        'hashtags': post.get('hashtags', [])\n",
    "    })\n",
    "\n",
    "posts_df = pd.DataFrame(posts_data)\n",
    "print(f\"posts_df shape: {posts_df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_identification shape: (64171, 2)\n",
      "emotion_df shape: (47890, 2)\n",
      "posts_df shape: (64171, 3)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:40:20.426727Z",
     "start_time": "2025-11-30T02:40:20.288606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ======================\n",
    "# 2. DATA TRANSFORMATION\n",
    "# ======================\n",
    "# Merge posts with split information\n",
    "print(\"\\n[2.1] Merging posts with split information...\")\n",
    "X = posts_df.merge(data_identification, on='id', how='left')\n",
    "print(f\"Merged dataframe shape: {X.shape}\")\n",
    "print(X.head())\n",
    "\n",
    "# Create train and test DataFrames\n",
    "print(\"\\n[2.2] Creating train and test DataFrames...\")\n",
    "train_df = X[X['split'] == 'train'].copy()\n",
    "test_df = X[X['split'] == 'test'].copy()\n",
    "\n",
    "# Add emotion labels to training data\n",
    "train_df = train_df.merge(emotion_df, on='id', how='left')\n",
    "\n",
    "print(f\"Shape of Training df: {train_df.shape}\")\n",
    "print(f\"Shape of Testing df: {test_df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2.1] Merging posts with split information...\n",
      "Merged dataframe shape: (64171, 4)\n",
      "         id                                               text hashtags  split\n",
      "0  0x61fc95  We got the ranch, loaded our guns and sat up t...       []   test\n",
      "1  0x35663e  I bet there is an army of married couples who ...       []  train\n",
      "2  0xc78afe                         This could only end badly.       []  train\n",
      "3  0x90089c  My sister squeezed a lime in her milk when she...       []  train\n",
      "4  0xaba820         and that got my head bobbing a little bit.       []   test\n",
      "\n",
      "[2.2] Creating train and test DataFrames...\n",
      "Shape of Training df: (47890, 5)\n",
      "Shape of Testing df: (16281, 4)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:41:07.484746Z",
     "start_time": "2025-11-30T02:41:07.418430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============\n",
    "# 3. DATA QUALITY\n",
    "# ===============\n",
    "# Check missing values\n",
    "print(\"\\n[3.1] Checking for missing values...\")\n",
    "print(\"Training set missing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nTest set missing values:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\n[3.2] Checking for duplicate texts...\")\n",
    "print(f\"Duplicate texts in training: {sum(train_df.duplicated('text'))}\")\n",
    "print(f\"Duplicate texts in test: {sum(test_df.duplicated('text'))}\")\n",
    "\n",
    "# Drop duplicates if any\n",
    "train_df.drop_duplicates(subset='text', keep='first', inplace=True)\n",
    "test_df.drop_duplicates(subset='text', keep='first', inplace=True)\n",
    "print(f\"After removing duplicates - Training: {len(train_df)}, Test: {len(test_df)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.1] Checking for missing values...\n",
      "Training set missing values:\n",
      "id          0\n",
      "text        0\n",
      "hashtags    0\n",
      "split       0\n",
      "emotion     0\n",
      "dtype: int64\n",
      "\n",
      "Test set missing values:\n",
      "id          0\n",
      "text        0\n",
      "hashtags    0\n",
      "split       0\n",
      "dtype: int64\n",
      "\n",
      "[3.2] Checking for duplicate texts...\n",
      "Duplicate texts in training: 0\n",
      "Duplicate texts in test: 0\n",
      "After removing duplicates - Training: 47890, Test: 16281\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:45:27.634424Z",
     "start_time": "2025-11-30T02:45:27.404620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# 4. EXPLORATORY DATA ANALYSIS\n",
    "# ============================\n",
    "import matplotlib.pyplot as plt\n",
    "# Group to find distribution\n",
    "print(\"\\n[4.1] Emotion distribution in training data:\")\n",
    "print(train_df.groupby(['emotion']).count()['text'])\n",
    "\n",
    "# Plot emotion distribution\n",
    "print(\"\\n[4.2] Plotting emotion distribution...\")\n",
    "labels = train_df['emotion'].unique()\n",
    "post_total = len(train_df)\n",
    "df1 = train_df.groupby(['emotion']).count()['text']\n",
    "df1_pct = df1.apply(lambda x: round(x*100/post_total, 3))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "plt.bar(df1.index, df1.values)\n",
    "plt.ylabel('Number of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution in Training Data')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('emotion_distribution.png', dpi=100)\n",
    "plt.show()\n",
    "print(\"Saved: emotion_distribution.png\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.1] Emotion distribution in training data:\n",
      "emotion\n",
      "anger       10694\n",
      "disgust      1183\n",
      "fear         2009\n",
      "joy         23797\n",
      "sadness      3926\n",
      "surprise     6281\n",
      "Name: text, dtype: int64\n",
      "\n",
      "[4.2] Plotting emotion distribution...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: emotion_distribution.png\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:46:23.115171Z",
     "start_time": "2025-11-30T02:46:15.995880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===================\n",
    "# 5. FEATURE CREATION\n",
    "# ===================\n",
    "# Tokenize using NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text using NLTK word_tokenize\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return []\n",
    "    try:\n",
    "        return word_tokenize(text.lower())\n",
    "    except:\n",
    "        return text.lower().split()\n",
    "\n",
    "print(\"\\n[5.1] Creating unigrams feature (tokenization)...\")\n",
    "train_df['unigrams'] = train_df['text'].apply(lambda x: tokenize_text(x))\n",
    "test_df['unigrams'] = test_df['text'].apply(lambda x: tokenize_text(x))\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample unigrams:\")\n",
    "print(train_df[['text', 'unigrams']].head(3))\n",
    "\n",
    "# Add text length feature\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "test_df['text_length'] = test_df['text'].str.len()\n",
    "\n",
    "# Add token count feature\n",
    "train_df['token_count'] = train_df['unigrams'].apply(len)\n",
    "test_df['token_count'] = test_df['unigrams'].apply(len)\n",
    "\n",
    "print(f\"\\nAverage text length (train): {train_df['text_length'].mean():.2f}\")\n",
    "print(f\"Average token count (train): {train_df['token_count'].mean():.2f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.1] Creating unigrams feature (tokenization)...\n",
      "Sample unigrams:\n",
      "                                                text  \\\n",
      "0  I bet there is an army of married couples who ...   \n",
      "1                         This could only end badly.   \n",
      "2  My sister squeezed a lime in her milk when she...   \n",
      "\n",
      "                                            unigrams  \n",
      "0  [i, bet, there, is, an, army, of, married, cou...  \n",
      "1                 [this, could, only, end, badly, .]  \n",
      "2  [my, sister, squeezed, a, lime, in, her, milk,...  \n",
      "\n",
      "Average text length (train): 75.24\n",
      "Average token count (train): 16.94\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:13:46.680560Z",
     "start_time": "2025-11-30T03:13:45.952913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================\n",
    "# 6 TEXT CLEANING\n",
    "# ================\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text data\n",
    "    Steps:\n",
    "    1. Lowercase conversion\n",
    "    2. Remove URLs\n",
    "    3. Remove mentions (@user)\n",
    "    4. Remove hashtag symbols (keep the text)\n",
    "    5. Remove special characters\n",
    "    6. Remove extra whitespace\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "\n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove hashtag symbol but keep the text\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "\n",
    "    # Remove special characters and numbers (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "print(\"\\n[5.2] Cleaning text data...\")\n",
    "train_df['text_cleaned'] = train_df['text'].apply(clean_text)\n",
    "test_df['text_cleaned'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample cleaned text:\")\n",
    "print(train_df[['text', 'text_cleaned']].head(3))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.2] Cleaning text data...\n",
      "Sample cleaned text:\n",
      "                                                text  \\\n",
      "0  I bet there is an army of married couples who ...   \n",
      "1                         This could only end badly.   \n",
      "2  My sister squeezed a lime in her milk when she...   \n",
      "\n",
      "                                        text_cleaned  \n",
      "0  i bet there is an army of married couples who ...  \n",
      "1                          this could only end badly  \n",
      "2  my sister squeezed a lime in her milk when she...  \n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:14:28.127480Z",
     "start_time": "2025-11-30T03:14:23.896672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===================\n",
    "# 7 STOPWORD REMOVAL\n",
    "# ===================\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"\\nNumber of stopwords: {len(stop_words)}\")\n",
    "print(f\"Sample stopwords: {list(stop_words)[:10]}\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text - Based on Lab 1\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "print(\"\\n[5.3] Removing stopwords...\")\n",
    "train_df['text_no_stopwords'] = train_df['text_cleaned'].apply(remove_stopwords)\n",
    "test_df['text_no_stopwords'] = test_df['text_cleaned'].apply(remove_stopwords)\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample after stopword removal:\")\n",
    "print(train_df[['text_cleaned', 'text_no_stopwords']].head(3))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of stopwords: 198\n",
      "Sample stopwords: ['our', 'theirs', \"we've\", 'having', 'will', \"don't\", \"mustn't\", 'through', \"she'd\", 'o']\n",
      "\n",
      "[5.3] Removing stopwords...\n",
      "Sample after stopword removal:\n",
      "                                        text_cleaned  \\\n",
      "0  i bet there is an army of married couples who ...   \n",
      "1                          this could only end badly   \n",
      "2  my sister squeezed a lime in her milk when she...   \n",
      "\n",
      "                                   text_no_stopwords  \n",
      "0               bet army married couples exact thing  \n",
      "1                                    could end badly  \n",
      "2  sister squeezed lime milk thing happened told ...  \n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:15:52.816581Z",
     "start_time": "2025-11-30T03:15:20.445813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============\n",
    "# 8 LEMMATIZATION\n",
    "# ===============\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Convert POS tag to WordNet format - Based on Lab 1\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default to noun\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatize text with POS tagging - Based on Lab 1\"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "                  for word, tag in pos_tags if len(word) > 1]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "print(\"\\n[5.4] Lemmatizing text...\")\n",
    "train_df['text_lemmatized'] = train_df['text_no_stopwords'].apply(lemmatize_text)\n",
    "test_df['text_lemmatized'] = test_df['text_no_stopwords'].apply(lemmatize_text)\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample after lemmatization:\")\n",
    "print(train_df[['text_no_stopwords', 'text_lemmatized']].head(3))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.4] Lemmatizing text...\n",
      "Sample after lemmatization:\n",
      "                                   text_no_stopwords  \\\n",
      "0               bet army married couples exact thing   \n",
      "1                                    could end badly   \n",
      "2  sister squeezed lime milk thing happened told ...   \n",
      "\n",
      "                                     text_lemmatized  \n",
      "0                  bet army marry couple exact thing  \n",
      "1                                    could end badly  \n",
      "2  sister squeeze lime milk thing happen tell wou...  \n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:15:58.901661Z",
     "start_time": "2025-11-30T03:15:58.892844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 9 FINAL PREPROCESSED TEXT\n",
    "# =============================================================================\n",
    "# Use the fully preprocessed text for feature engineering\n",
    "train_df['text_preprocessed'] = train_df['text_lemmatized']\n",
    "test_df['text_preprocessed'] = test_df['text_lemmatized']\n",
    "\n",
    "print(\"\\n[5.5] Preprocessing complete!\")\n",
    "print(f\"Original: {train_df['text'].iloc[0][:80]}...\")\n",
    "print(f\"Preprocessed: {train_df['text_preprocessed'].iloc[0][:80]}...\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.5] Preprocessing complete!\n",
      "Original: I bet there is an army of married couples who did the same exact thing....\n",
      "Preprocessed: bet army marry couple exact thing...\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:22:23.721846Z",
     "start_time": "2025-11-30T03:22:08.676917Z"
    }
   },
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "# =====================================\n",
    "# 10. FEATURE ENGINEERING - Bag of Words\n",
    "# =====================================\n",
    "# Build BOW vectorizer\n",
    "print(\"\\n[6.1] Building CountVectorizer (Bag of Words)...\")\n",
    "BOW_vectorizer = CountVectorizer(\n",
    "    max_features=5000,           # Limit vocabulary size\n",
    "    tokenizer=nltk.word_tokenize # Use NLTK tokenizer\n",
    ")\n",
    "\n",
    "# Learn vocabulary and transform\n",
    "BOW_vectorizer.fit(train_df['text'])\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_df['text'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(f\"BOW training matrix shape: {train_data_BOW_features.shape}\")\n",
    "print(f\"BOW test matrix shape: {test_data_BOW_features.shape}\")\n",
    "\n",
    "# Show sample feature names\n",
    "feature_names_bow = BOW_vectorizer.get_feature_names_out()\n",
    "print(f\"Sample feature names [100:110]: {list(feature_names_bow[100:110])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6.1] Building CountVectorizer (Bag of Words)...\n",
      "BOW training matrix shape: (47890, 5000)\n",
      "BOW test matrix shape: (16281, 5000)\n",
      "Sample feature names [100:110]: ['90', '90s', '95', '99', ':', ':3', ';', '<', '=', '>']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:23:01.375149Z",
     "start_time": "2025-11-30T03:22:59.545843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============================\n",
    "# 11. FEATURE ENGINEERING - TF-IDF\n",
    "# ===============================\n",
    "# Build TF-IDF vectorizer\n",
    "print(\"\\n[7.1] Building TfidfVectorizer...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',                    # Remove stopwords\n",
    "    token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b',   # Only alphabetic tokens, min 2 chars\n",
    "    max_features=10000,                       # Vocabulary size\n",
    "    ngram_range=(1, 2),                       # Unigrams and bigrams\n",
    "    min_df=2,                                 # Minimum document frequency\n",
    "    max_df=0.95                               # Maximum document frequency\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['text'].fillna('').astype(str))\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['text'].fillna('').astype(str))\n",
    "\n",
    "print(f\"TF-IDF training matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF test matrix shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Show sample feature names\n",
    "feat_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Sample feature names [100:110]: {list(feat_names_tfidf[100:110])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7.1] Building TfidfVectorizer...\n",
      "TF-IDF training matrix shape: (47890, 10000)\n",
      "TF-IDF test matrix shape: (16281, 10000)\n",
      "Sample feature names [100:110]: ['actually say', 'actually sounds', 'actually think', 'actually thought', 'ad', 'ad hominem', 'add', 'added', 'addict', 'addicted']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:24:54.225288Z",
     "start_time": "2025-11-30T03:24:44.513029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =======================\n",
    "# 12. WORD2VEC EMBEDDINGS\n",
    "# =======================\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Prepare training corpus\n",
    "print(\"\\nPreparing training corpus...\")\n",
    "train_df['text_tokenized'] = train_df['text_preprocessed'].apply(lambda x: word_tokenize(x) if x else [])\n",
    "training_corpus = train_df['text_tokenized'].values.tolist()\n",
    "print(f\"Training corpus size: {len(training_corpus)}\")\n",
    "print(f\"Sample: {training_corpus[0][:10]}\")\n",
    "\n",
    "# Train Word2Vec model\n",
    "print(\"\\nTraining Word2Vec model...\")\n",
    "vector_dim = 100\n",
    "window_size = 5\n",
    "min_count = 2         # Words must appear at least 2 times\n",
    "training_epochs = 20\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=training_corpus,\n",
    "    vector_size=vector_dim,\n",
    "    window=window_size,\n",
    "    min_count=min_count,\n",
    "    epochs=training_epochs,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "print(f\"Word2Vec vocabulary size: {len(word2vec_model.wv)}\")\n",
    "\n",
    "# Create sentence embeddings using mean pooling\n",
    "print(\"\\nCreating sentence embeddings (mean pooling)...\")\n",
    "\n",
    "def get_sentence_vector(tokens, model, dim=100):\n",
    "    \"\"\"\n",
    "    Convert sentence to vector by averaging word vectors\n",
    "    Based on Lab 2 Phase 1, Exercise 7\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vectors.append(model.wv[token])\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Create Word2Vec features\n",
    "X_train_w2v = np.array([\n",
    "    get_sentence_vector(tokens, word2vec_model, vector_dim)\n",
    "    for tokens in train_df['text_tokenized']\n",
    "])\n",
    "\n",
    "test_df['text_tokenized'] = test_df['text_preprocessed'].apply(lambda x: word_tokenize(x) if x else [])\n",
    "X_test_w2v = np.array([\n",
    "    get_sentence_vector(tokens, word2vec_model, vector_dim)\n",
    "    for tokens in test_df['text_tokenized']\n",
    "])\n",
    "\n",
    "print(f\"Word2Vec training features shape: {X_train_w2v.shape}\")\n",
    "print(f\"Word2Vec test features shape: {X_test_w2v.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training corpus...\n",
      "Training corpus size: 47890\n",
      "Sample: ['bet', 'army', 'marry', 'couple', 'exact', 'thing']\n",
      "\n",
      "Training Word2Vec model...\n",
      "Word2Vec vocabulary size: 12433\n",
      "\n",
      "Creating sentence embeddings (mean pooling)...\n",
      "Word2Vec training features shape: (47890, 100)\n",
      "Word2Vec test features shape: (16281, 100)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:26:02.772011Z",
     "start_time": "2025-11-30T03:25:20.671981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 13. PRE-TRAINED EMBEDDINGS\n",
    "# ==========================\n",
    "import gensim.downloader as api\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# Fix SSL certificate issue\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "print(\"\\nLoading pre-trained GloVe model (Twitter, 25-dim)...\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "try:\n",
    "    glove_model = api.load(\"glove-twitter-25\")\n",
    "    glove_dim = 25\n",
    "    print(f\"GloVe model loaded! Vocabulary size: {len(glove_model)}\")\n",
    "\n",
    "    # Test the model\n",
    "    print(f\"Most similar to 'happy': {glove_model.most_similar('happy', topn=5)}\")\n",
    "\n",
    "    # 9.2 Create GloVe sentence embeddings\n",
    "    print(\"\\nCreating GloVe sentence embeddings...\")\n",
    "\n",
    "    def get_glove_vector(tokens, model, dim=25):\n",
    "        \"\"\"Get sentence embedding using pre-trained GloVe\"\"\"\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model:\n",
    "                vectors.append(model[token])\n",
    "\n",
    "        if len(vectors) == 0:\n",
    "            return np.zeros(dim)\n",
    "\n",
    "        return np.mean(vectors, axis=0)\n",
    "\n",
    "    X_train_glove = np.array([\n",
    "        get_glove_vector(tokens, glove_model, glove_dim)\n",
    "        for tokens in train_df['text_tokenized']\n",
    "    ])\n",
    "\n",
    "    X_test_glove = np.array([\n",
    "        get_glove_vector(tokens, glove_model, glove_dim)\n",
    "        for tokens in test_df['text_tokenized']\n",
    "    ])\n",
    "\n",
    "    print(f\"GloVe training features shape: {X_train_glove.shape}\")\n",
    "    print(f\"GloVe test features shape: {X_test_glove.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load GloVe model: {e}\")\n",
    "    print(\"Skipping GloVe features...\")\n",
    "    X_train_glove = None\n",
    "    X_test_glove = None"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pre-trained GloVe model (Twitter, 25-dim)...\n",
      "This may take a few minutes on first run...\n",
      "[===========---------------------------------------] 22.4% 23.4/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 60.5% 63.4/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.3% 95.7/104.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GloVe model loaded! Vocabulary size: 1193514\n",
      "Most similar to 'happy': [('birthday', 0.9577818512916565), ('thank', 0.937666654586792), ('welcome', 0.93361496925354), ('love', 0.9176183342933655), ('miss', 0.9164500832557678)]\n",
      "\n",
      "Creating GloVe sentence embeddings...\n",
      "GloVe training features shape: (47890, 25)\n",
      "GloVe test features shape: (16281, 25)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:20:22.773444Z",
     "start_time": "2025-11-30T03:20:22.750409Z"
    }
   },
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "# =================\n",
    "# . PREPARE LABELS\n",
    "# =================\n",
    "# Define emotion mapping (for Kaggle submission)\n",
    "emotion_mapping = {\n",
    "    'anger': 0,\n",
    "    'disgust': 1,\n",
    "    'fear': 2,\n",
    "    'joy': 3,\n",
    "    'sadness': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "reverse_mapping = {v: k for k, v in emotion_mapping.items()}\n",
    "\n",
    "# Encode labels\n",
    "train_df['emotion_encoded'] = train_df['emotion'].map(emotion_mapping)\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = X_train_tfidf\n",
    "y_train = train_df['emotion']  # String labels for sklearn\n",
    "y_train_encoded = train_df['emotion_encoded']  # Numeric labels\n",
    "\n",
    "X_test = X_test_tfidf\n",
    "\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\"X_test.shape: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(y_train.value_counts())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (47890, 10000)\n",
      "y_train.shape: (47890,)\n",
      "X_test.shape: (16281, 10000)\n",
      "\n",
      "Label distribution:\n",
      "emotion\n",
      "joy         23797\n",
      "anger       10694\n",
      "surprise     6281\n",
      "sadness      3926\n",
      "fear         2009\n",
      "disgust      1183\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
